<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="James S. Clark" />

<meta name="date" content="2016-11-29" />

<title>Generalized joint attribute modeling - gjam</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Generalized joint attribute modeling - gjam</h1>
<h4 class="author"><em><a href="http://sites.nicholas.duke.edu/clarklab/">James S. Clark</a></em></h4>
<h4 class="date"><em>2016-11-29</em></h4>


<div id="TOC">
<ul>
<li><a href="#overview">Overview</a><ul>
<li><a href="#model-summary">Model summary</a></li>
<li><a href="#model-interpretation">Model interpretation</a></li>
<li><a href="#summary-of-data-types">Summary of data types</a></li>
<li><a href="#effort-and-weight-of-discrete-data">Effort and weight of discrete data</a></li>
</ul></li>
<li><a href="#using-gjam">Using gjam</a><ul>
<li><a href="#outputs">Outputs</a></li>
<li><a href="#examples-from-simulation">Examples from simulation</a></li>
<li><a href="#my-data">My data</a></li>
</ul></li>
<li><a href="#plotting-output">Plotting output</a></li>
<li><a href="#flexibility-in-gjam">Flexibility in gjam</a><ul>
<li><a href="#heterogeneous-sample-effort">Heterogeneous sample effort</a></li>
<li><a href="#prior-distribution-on-coefficients">Prior distribution on coefficients</a></li>
<li><a href="#sample-effort-in-composition-data">Sample effort in composition data</a></li>
<li><a href="#the-partition-in-ordinal-data">The partition in ordinal data</a></li>
<li><a href="#categorical-data">Categorical data</a></li>
<li><a href="#combinations-of-data-types">Combinations of data types</a></li>
<li><a href="#missing-data-out-of-sample-prediction">Missing data, out-of-sample prediction</a></li>
<li><a href="#prediction-with-heterogenous-effort">Prediction with heterogenous effort</a></li>
<li><a href="#conditional-prediction">Conditional prediction</a></li>
<li><a href="#presence-only-data-with-effort">Presence-only data with effort</a></li>
</ul></li>
<li><a href="#when-a-model-wont-execute">When a model wonâ€™t execute</a></li>
<li><a href="#reference-notes">Reference notes</a><ul>
<li><a href="#parameter-dimensions">Parameter dimensions</a></li>
<li><a href="#when-there-are-factors-in-mathbfx">When there are factors in <span class="math inline">\(\mathbf{X}\)</span></a></li>
<li><a href="#algorithm-summary">Algorithm summary</a></li>
</ul></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="citation" class="section level4">
<h4>citation:</h4>
<p><em>Clark, J.S., D. Nemergut, B. Seyednasrollah, P. Turner, and S. Zhang. 2016. Generalized joint attribute modeling for biodiversity analysis: Median-zero, multivariate, multifarious data, Ecological Monographs, in press.</em></p>
<p>files are found <a href="http://sites.nicholas.duke.edu/clarklab/code/">here</a></p>
</div>
<div id="gjam-vignettes" class="section level4">
<h4>gjam vignettes:</h4>
<ol style="list-style-type: decimal">
<li><em>Generalized joint attribute modeling - gjam</em>: <strong>this overview</strong></li>
<li><em>Dimension reduction in gjam</em>: application to many response variables (â€˜Big-Sâ€™)</li>
<li><em>Trait modeling in gjam</em>: ecological trait analysis</li>
</ol>
</div>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p><code>gjam</code> models multivariate responses that can be combinations of discrete and continuous variables, where interpretation is needed on the observation scale. It was motivated by the challenges of modeling distribution and abundance of multiple species, so-called joint species distribution models (JSDMs), where species and other attributes are recorded on different scales. Some species groups are counted. Some may be continuous cover values or basal area. Some may be recorded in ordinal bins, such as â€˜rareâ€™, â€˜moderateâ€™, and â€˜abundantâ€™. Others may be presence-absence. Some are composition data, either fractional (continuous on (0, 1)) or counts (e.g., molecular and fossil pollen data). Attributes such as body condition, infection status, and herbivore damage are often included in field data. To allow transparent interpretation <code>gjam</code> avoids non-linear link functions.</p>
<p>The integration of discrete and continuous data on the observed scales makes use of <em>censoring</em>. Censoring extends a model for continuous variables across censored intervals. Continuous observations are uncensored. Censored observations are discrete and can depend on sample effort.</p>
<p>Censoring is used with the <em>effort</em> for an observation to combine continuous and discrete variables with appropriate weight. In count data, effort is determined by the size of the sample plot, search time, or both. It is comparable to the offset in generalized linear models (GLM). In count composition data, effort is the total count taken over all species. In PCR, effort is the number of reads for the sample. In paleoecological data it is the count for the sample. In <code>gjam</code> discrete observations can be viewed as censored versions of an underlying continuous space.</p>
<div id="model-summary" class="section level3">
<h3>Model summary</h3>
<p>The basic model is detailed in Clark et al. (2016). An observation consists of environmental variables and species attributes, <span class="math inline">\(\lbrace \mathbf{x}_{i}, \mathbf{y}_{i}\rbrace\)</span>, <span class="math inline">\(i = 1,..., n\)</span>. The vector <span class="math inline">\(\mathbf{x}_{i}\)</span> contains predictors <span class="math inline">\(x_{iq}: q = 1,..., Q\)</span>. The vector <span class="math inline">\(\mathbf{y}_{i}\)</span> contains attributes (responses), such as species abundance, presence-absence, and so forth, <span class="math inline">\(y_{is}: s = 1,..., S\)</span>. The effort <span class="math inline">\(E_{is}\)</span> invested to obtain the observation of response <span class="math inline">\(s\)</span> at location <span class="math inline">\(i\)</span> can affect the observation. The combinations of continuous and discrete measurements in observed <span class="math inline">\(\mathbf{y}_{i}\)</span> motivate the three elements of <code>gjam</code>.</p>
<p>A length-<span class="math inline">\(S\)</span> vector <span class="math inline">\(\mathbf{w}_{i}\in{\Re}^S\)</span> represents response <span class="math inline">\(\mathbf{y}_i\)</span> in continuous space. This continuous space allows for the dependence structure with a covariance matrix. An element <span class="math inline">\(w_{is}\)</span> can be known (e.g., continuous response <span class="math inline">\(y_{is}\)</span>) or unknown (e.g., discrete responses).</p>
<p>A length-<span class="math inline">\(S\)</span> vector of integers <span class="math inline">\(\mathbf{z}_{i}\)</span> represents <span class="math inline">\(\mathbf{y}_i\)</span> in discrete space. Each observed <span class="math inline">\(y_{is}\)</span> is assigned to an interval <span class="math inline">\(z_{is} \in \{0,...,K_{is}\}\)</span>. The number of intervals <span class="math inline">\(K_{is}\)</span> can differ between observations and between species, because each species can be observed in different ways.</p>
<p>The partition of continuous space at points <span class="math inline">\(p_{is,z} \in{\mathcal{P}}\)</span> defines discrete intervals <span class="math inline">\(z_{is}\)</span>. Two values <span class="math inline">\((p_{is,k}, p_{is,k+1}]\)</span> bound the <span class="math inline">\(k^{th}\)</span> interval of <span class="math inline">\(s\)</span> in observation <span class="math inline">\(i\)</span>. Intervals are contiguous and provide support over the real line <span class="math inline">\((-\infty, \infty)\)</span>. For discrete observations, <span class="math inline">\(k\)</span> is a censored interval, and <span class="math inline">\(w_{is}\)</span> is a latent variable. The set of censored intervals is <span class="math inline">\(\mathcal{C}\)</span>. The partition set <span class="math inline">\(\mathcal{P}\)</span> can include both known (discrete counts, including composition data) and unknown (ordinal, categorical) points.</p>
<p>An observation <span class="math inline">\(y\)</span> maps to <span class="math inline">\(w\)</span>,</p>
<p><span class="math display">\[y_{is} = \left \{
\begin{matrix}
\ w_{is} &amp; continuous\\
\ z_{is}, &amp; w_{is} \in (p_{z_{is}}, p_{z_{is} + 1}] &amp; discrete
\end{matrix}
\right.\]</span></p>
<p>Effort <span class="math inline">\(E_{is}\)</span> affects the partition for discrete data. For the simple case where there is no error in the assignment of discrete intervals, <span class="math inline">\(\mathbf{z}_i\)</span> is known, and the model for <span class="math inline">\(\mathbf{w}_i\)</span> is</p>
<p><span class="math display">\[\mathbf{w}_i|\mathbf{x}_i, \mathbf{y}_i, \mathbf{E}_i \sim MVN(\boldsymbol{\mu}_i,\boldsymbol{\Sigma}) \times \prod_{s=1}^S\mathcal{I}_{is}\]</span> <span class="math display">\[\boldsymbol{\mu}_i = \mathbf{B}'\mathbf{x}_i\]</span> <span class="math display">\[\mathcal{I}_{is} = \prod_{k \in \mathcal{C}}I_{is,k}^{I(y_{is} = k)} (1 - I_{is,k})^{I(y_{is} \neq k)}\]</span></p>
<p>where <span class="math inline">\(I_{is} =I(p_{z_{is}} &lt; w_{is} &lt; p_{z_{is} + 1}]\)</span>, <span class="math inline">\(\mathcal{C}\)</span> is the set of discrete intervals, <span class="math inline">\(\mathbf{B}\)</span> is a <span class="math inline">\(Q \times S\)</span> matrix of coefficients, and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is a <span class="math inline">\(S \times S\)</span> covariance matrix. There is a correlation matrix associated with <span class="math inline">\(\boldsymbol{\Sigma}\)</span>,</p>
<p><span class="math display">\[\mathbf{R}_{s,s'} = \frac{\boldsymbol{\Sigma}_{s,s'}}{\sqrt{\boldsymbol{\Sigma}_{s,s} \boldsymbol{\Sigma}_{s',s'}}}\]</span></p>
</div>
<div id="model-interpretation" class="section level3">
<h3>Model interpretation</h3>
<p><em>As a data-generating mechanism the model can be thought of like this</em>: There is a vector of continuous responses <span class="math inline">\(\mathbf{w}_{i}\)</span> generated from mean vector <span class="math inline">\(\boldsymbol{\mu}_{i}\)</span> and covariance <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (Fig. 1a). The partition <span class="math inline">\(\mathbf{p}_{is} = (-\infty, \dots, \infty)\)</span> segments the real line into intervals, some of which are censored and others not. Each interval is defined by two values, <span class="math inline">\((p_{is,k}, p_{is,k+1}]\)</span>. For a value of <span class="math inline">\(w_{is}\)</span> that falls within a censored interval <span class="math inline">\(k\)</span> the observed <span class="math inline">\(y_{is}\)</span> is assigned to discrete interval <span class="math inline">\(z_{is} = k\)</span>. For a value of <span class="math inline">\(w_{is}\)</span> that falls in an uncensored interval <span class="math inline">\(y_{is}\)</span> is assigned <span class="math inline">\(w_{is}\)</span>.</p>
<p>Of course, data present us with the inverse problem: the observed <span class="math inline">\(y_{is}\)</span> are continuous or discrete, with known or unknown partition <span class="math inline">\((p_{is,k}, p_{is,k+1}]\)</span> (Fig. 1b). Depending on how the data are observed, we must impute the elements of <span class="math inline">\(n \times S\)</span> matrix <span class="math inline">\(\mathbf{W}\)</span> that lie within censored intervals. Unknown elements of <span class="math inline">\(\mathcal{P}\)</span> will also be imputed in order to estimate <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<p><img src="data:image/png;base64,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" alt /><!-- --> <strong>Figure 1. Censoring in gjam.</strong> As a data-generating model (a), a realization <span class="math inline">\(w_{is}\)</span> that lies within a censored interval is translated by the partition <span class="math inline">\(\mathbf{p}_{is}\)</span> to discrete <span class="math inline">\(y_{is}\)</span>. The distribution of data (bars at left) is induced by the latent scale and the partition. For inference (b), observed discrete <span class="math inline">\(y_{is}\)</span> takes values on the latent scale from a truncated distribution.</p>
</div>
<div id="summary-of-data-types" class="section level3">
<h3>Summary of data types</h3>
<p>The different types of data that can be included in the model are summarized in Table 1, assigned to the <code>character</code> variable <code>typeNames</code> that is included in the <code>modelList</code> passed to <code>gjamGibbs</code>:</p>
<p><strong>Table 1. Partition for each data type</strong></p>
<table style="width:101%;">
<colgroup>
<col width="9%"></col>
<col width="15%"></col>
<col width="22%"></col>
<col width="15%"></col>
<col width="38%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="center"><code>typeNames</code></th>
<th align="center">Type</th>
<th align="center">Obs values</th>
<th align="center">Default partition</th>
<th align="left">Comments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>'CON'</code></td>
<td align="center">continuous, uncensored</td>
<td align="center"><span class="math inline">\((-\infty, \infty)\)</span></td>
<td align="center">none</td>
<td align="left">e.g., centered, standardized</td>
</tr>
<tr class="even">
<td align="center"><code>'CA'</code></td>
<td align="center">continuous abundance</td>
<td align="center"><span class="math inline">\([0, \infty)\)</span></td>
<td align="center"><span class="math inline">\((-\infty, 0, \infty)\)</span></td>
<td align="left">with zeros</td>
</tr>
<tr class="odd">
<td align="center"><code>'DA'</code></td>
<td align="center">discrete abundance</td>
<td align="center"><span class="math inline">\(\{0, 1, 2, \dots \}\)</span></td>
<td align="center"><span class="math inline">\((-\infty, \frac{1}{2E_{i}}, \frac{3}{2E_{i}}, \dots , \frac{max_s(y_{is}) - 1/2}{E_i}, \infty)^1\)</span></td>
<td align="left">e.g., count data</td>
</tr>
<tr class="even">
<td align="center"><code>'PA'</code></td>
<td align="center">presence- absence</td>
<td align="center"><span class="math inline">\(\{0, 1\}\)</span></td>
<td align="center"><span class="math inline">\((-\infty, 0, \infty)\)</span></td>
<td align="left">unit variance scale</td>
</tr>
<tr class="odd">
<td align="center"><code>'OC'</code></td>
<td align="center">ordinal counts</td>
<td align="center"><span class="math inline">\(\{0, 1, 2, \dots , K \}\)</span></td>
<td align="center"><span class="math inline">\((-\infty, 0, estimates, \infty)\)</span></td>
<td align="left">unit variance scale, imputed partition</td>
</tr>
<tr class="even">
<td align="center"><code>'FC'</code></td>
<td align="center">fractional composition</td>
<td align="center"><span class="math inline">\([0, 1]\)</span></td>
<td align="center"><span class="math inline">\((-\infty, 0, 1, \infty)\)</span></td>
<td align="left">relative abundance</td>
</tr>
<tr class="odd">
<td align="center"><code>'CC'</code></td>
<td align="center">count composition</td>
<td align="center"><span class="math inline">\(\{0, 1, 2, \dots \}\)</span></td>
<td align="center"><span class="math inline">\((-\infty, \frac{1}{2E_{i}}, \frac{3}{2E_{i}}, \dots , 1 - \frac{1}{2E_i}, \infty)^1\)</span></td>
<td align="left">relative abundance counts</td>
</tr>
<tr class="even">
<td align="center"><code>'CAT'</code></td>
<td align="center">categorical</td>
<td align="center"><span class="math inline">\(\{0, 1\}\)</span></td>
<td align="center"><span class="math inline">\((-\infty, max_{k}(0, w_{is,k}), \infty)^2\)</span></td>
<td align="left">unit variance, multiple levels</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(^1\)</span> For <code>'DA'</code> and <code>'CC'</code> data the second element of the partition is not zero, but rather depends on effort. There is thus zero-inflation. The default partition for each data type can be changed with the function <code>gjamCensorY</code> (see <strong>Specifying censored intervals</strong>).<br />
<span class="math inline">\(^2\)</span> For <code>'CAT'</code> data species <span class="math inline">\(s\)</span> has <span class="math inline">\(K_s - 1\)</span> non-reference categories. The category with the largest <span class="math inline">\(w_{is,k}\)</span> is the â€˜1â€™, all others are zeros.</p>
<p><br></p>
<p>For <strong>presence-absence</strong> (binary) data, <span class="math inline">\(\mathbf{p}_{is} = (-\infty, 0, \infty)\)</span>. This is equivalent to Chib and Greenbergâ€™s (2008) model, which could be written <span class="math inline">\(\mathcal{I}_{is} = I(w_{is} &gt; 0)^{y_{is}}I(w_{is} \leqslant 0)^{1 - y_{is}}\)</span>.</p>
<p>For a continous variable with point mass at zero, <strong>continuous abundance</strong>, this is a multivariate Tobit model, with <span class="math inline">\(\mathcal{I}_{is} = I(w_{is} = y_{is})^{I(y_{is} &gt; 0)}I(w_{is} \leqslant 0)^{I(y_{is} = 0)}\)</span>. This is the same partition used for the probit model, the difference being that the positive values in the Tobit are uncensored.</p>
<p><strong>Categorical</strong> responses fit within the same framework. Each categorical response occupies as many columns in <span class="math inline">\(\mathbf{Y}\)</span> as there are independent levels in response <span class="math inline">\(s\)</span>, levels being <span class="math inline">\(k = 1,..., K_{s}-1\)</span>. For example, if randomly sampled plots are scored by one of five cover types, then there are four columns in <span class="math inline">\(\mathbf{Y}\)</span> for the response <span class="math inline">\(s\)</span>. The four columns can have at most one <span class="math inline">\(1\)</span>. If all four columns are <span class="math inline">\(0\)</span>, then the reference level is observed. The observed level has the largest value of <span class="math inline">\(w_{is,k}\)</span> (Table 1). This is similar to Zhang et al.â€™s (2008) model for categorical data.</p>
<p>For <strong>ordinal counts</strong> gjam is Lawrence et al.â€™s (2008) model having the partition <span class="math inline">\(\mathbf{p}_{is} = (-\infty, 0, p_{is,2}, p_{is,3},..., p_{is,K}, \infty)\)</span>, where all but the first two and the last elements must be inferred. The partition must be inferred, because the ordinal scale is only relative.</p>
<p>Like categorical data, <strong>composition</strong> data have one reference class. For this and other <strong>discrete count</strong> data, the partition for observation <span class="math inline">\(i\)</span> can be defined to account for sample effort (next section).</p>
</div>
<div id="effort-and-weight-of-discrete-data" class="section level3">
<h3>Effort and weight of discrete data</h3>
<p>The partition for a discrete interval <span class="math inline">\(k\)</span> depends on effort for sample <span class="math inline">\(i\)</span></p>
<p><span class="math display">\[(p_{i,k}, p_{i,k+1}] = \left(\frac{k - 1/2}{E_{i}}, \frac{k + 1/2}{E_{i}}\right]\]</span></p>
<p>Effort affects the partition and, thus, the weight of each observation; wide intervals allow large variance, and vice versa. For <em>discrete abundance</em> (<code>'DA'</code>) data on plots of a given area, large plots contribute more weight than small plots. Because plots have different areas one might choose to model <span class="math inline">\(w_{is}\)</span> on a â€˜per-areaâ€™ scale (density) rather than a â€˜per-plotâ€™ scale. If so, plot area becomes the â€˜effortâ€™. Here is a table of variables for the case where counts represent the same density of trees per area, but have different effort due to different plot areas:</p>
<table style="width:107%;">
<colgroup>
<col width="26%"></col>
<col width="20%"></col>
<col width="29%"></col>
<col width="6%"></col>
<col width="23%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="right">count <span class="math inline">\(y_{is} = z_{is}\)</span></th>
<th align="center">plot area <span class="math inline">\(E_{i}\)</span></th>
<th align="center">density <span class="math inline">\(w_{is}\)</span></th>
<th align="right">bin <span class="math inline">\(k\)</span></th>
<th align="center">density <span class="math inline">\(\mathbf{p}_{ik}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">10</td>
<td align="center">0.1 ha</td>
<td align="center">100 ha<span class="math inline">\(^{-1}\)</span></td>
<td align="right">11</td>
<td align="center">(95, 105]</td>
</tr>
<tr class="even">
<td align="right">100</td>
<td align="center">1.0 ha</td>
<td align="center">100 ha<span class="math inline">\(^{-1}\)</span></td>
<td align="right">101</td>
<td align="center">(99.5, 100.5]</td>
</tr>
</tbody>
</table>
<p>The wide partition on the 0.1-ha plot admits large variance around the observation of 10 trees per 0.1 ha plot. Wide variance on an observation decreases its contribution to the fit. Conversely, the narrow partition on the 1.0-ha plot constrains density to a narrow interval around the observed value.</p>
<p>For <em>composition count</em> (<code>'CC'</code>) data effort is represented by the total count. For <span class="math inline">\(0 &lt; y_{is} &lt; E_i\)</span> the variable <span class="math inline">\(0 &lt; w_{is} &lt; 1\)</span>, i.e., the composition scale. Using the same partition as previously the table for two observations that represent the fraction 0.10 with different effort (e.g., total reads in PCR data) looks like this:</p>
<table style="width:107%;">
<colgroup>
<col width="26%"></col>
<col width="19%"></col>
<col width="22%"></col>
<col width="11%"></col>
<col width="27%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="right">count <span class="math inline">\(y_{is} = z_{is}\)</span></th>
<th align="right">total count <span class="math inline">\(E_{i}\)</span></th>
<th align="center">fraction <span class="math inline">\(w_{is}\)</span></th>
<th align="right">bin <span class="math inline">\(k\)</span></th>
<th align="center">fraction <span class="math inline">\(\mathbf{p}_{ik}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">10</td>
<td align="right">100</td>
<td align="center">0.1</td>
<td align="right">11</td>
<td align="center">(0.095, 0.105]</td>
</tr>
<tr class="even">
<td align="right">10,000</td>
<td align="right">100,000</td>
<td align="center">0.1</td>
<td align="right">10,001</td>
<td align="center">(0.099995, 0.100005]</td>
</tr>
</tbody>
</table>
<p>Again, on the composition scale <span class="math inline">\([0, 1]\)</span>, weight of the observation is determined by the partition width and, in turn, effort.</p>
</div>
</div>
<div id="using-gjam" class="section level2">
<h2>Using gjam</h2>
<p>Itâ€™s easiest to start with the examples from <code>gjam</code> help pages. The first section, <strong>Simulated examples</strong>, expands on these <code>help</code> pages. The section that follows, <strong>My data</strong>, discusses some of the issues you might encounter when specifying your own model applied to your data.</p>
<div id="outputs" class="section level3">
<h3>Outputs</h3>
<p>Simulated data are used to check that the algorithm can recover true parameter values and predict data, including underlying latent variables. To illustrate I simulate a sample of size <span class="math inline">\(n = 500\)</span> for <span class="math inline">\(S = 10\)</span> species and <span class="math inline">\(Q = 3\)</span> predictors. To indicate that all species are continuous abundance data I specify <code>typeNames</code> as <code>'CA'</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gjam)
f &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">n =</span> <span class="dv">500</span>, <span class="dt">S =</span> <span class="dv">10</span>, <span class="dt">Q =</span> <span class="dv">3</span>, <span class="dt">typeNames =</span> <span class="st">'CA'</span>)
<span class="kw">summary</span>(f)</code></pre></div>
<p>The object <code>f</code> includes elements needed to analyze the simulated data set. <code>f$typeNames</code> is a length-<span class="math inline">\(S\)</span> <code>character vector</code>. The <code>formula</code> follows standard R syntax. It does not start with <code>y ~</code>, because gjam is multivariate. The multivariate response is supplied as a <span class="math inline">\(n \times S\)</span> <code>matrix</code> or <code>data.frame ydata</code>. Here is the formula for this example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f$formula</code></pre></div>
<p>The model can include interactions.</p>
<p>The simulated parameter values are returned from <code>gjamSimData</code> in the list <code>f$trueValues</code>, shown in Table 2 with the corresponding names of estimates from <code>gjamGibbs</code>:</p>
<p><strong>Table 2. Variable names and scales in simulation and fitting</strong></p>
<table style="width:110%;">
<colgroup>
<col width="11%"></col>
<col width="37%"></col>
<col width="9%"></col>
<col width="13%"></col>
<col width="37%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="center">model</th>
<th align="center"><code>$trueValues</code><span class="math inline">\(^{1}\)</span></th>
<th align="center"><code>$parameterTables</code><span class="math inline">\(^{2}\)</span></th>
<th align="center"><code>$chains</code><span class="math inline">\(^{2}\)</span></th>
<th align="left">scale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathbf{B}_{u, Q \times S}\)</span></td>
<td align="center"><code>beta</code></td>
<td align="center"><code>betaMu</code></td>
<td align="center"><code>bgibbs</code></td>
<td align="left"><span class="math inline">\(W/X\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\boldsymbol{\Sigma}_{S \times S}\)</span></td>
<td align="center"><code>sigma</code></td>
<td align="center"><code>sigMu</code></td>
<td align="center"><code>sgibbs</code></td>
<td align="left"><span class="math inline">\(W_{s}W_{s'}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathbf{R}_{S \times S}\)</span></td>
<td align="center"><code>corSpec</code></td>
<td align="center"><code>corMu</code></td>
<td align="center"></td>
<td align="left">correlation</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\tilde{\mathbf{B}}_{Q_1 \times S}\)</span></td>
<td align="center">-</td>
<td align="center"><code>fBetaMu</code></td>
<td align="center"><code>fbgibbs</code></td>
<td align="left">dimensionless</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathbf{f}_{Q_1}^3\)</span></td>
<td align="center">-</td>
<td align="center"><code>fMu</code></td>
<td align="center"><code>fgibbs</code></td>
<td align="left">dimensionless</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mathbf{F}_{Q_1 \times Q_1}^3\)</span></td>
<td align="center">-</td>
<td align="center"><code>fmatrix</code></td>
<td align="center"></td>
<td align="left">dimensionless</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathbf{E}_{S \times S}\)</span></td>
<td align="center">-</td>
<td align="center"><code>ematrix</code></td>
<td align="center">-</td>
<td align="left">dimensionless</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mathcal{P}\)</span> <span class="math inline">\(^4\)</span></td>
<td align="center"><code>cuts</code></td>
<td align="center"><code>cutMu</code></td>
<td align="center"><code>cgibbs</code></td>
<td align="left">correlation</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(K\)</span> <span class="math inline">\(^5\)</span></td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center"><code>kgibbs</code></td>
<td align="left">dimensionless</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\sigma^{2}\)</span> <span class="math inline">\(^5\)</span></td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center"><code>sigErrGibbs</code></td>
<td align="left"><span class="math inline">\(W^2\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\boldsymbol{\alpha}_{Q \times M}\)</span> <span class="math inline">\(^6\)</span></td>
<td align="center">-</td>
<td align="center"><code>betaTraitMu</code></td>
<td align="center"><code>agibbs</code></td>
<td align="left"><span class="math inline">\(U/X\)</span> <span class="math inline">\(^6\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\boldsymbol{\Omega}_{M \times M}\)</span> <span class="math inline">\(^6\)</span></td>
<td align="center">-</td>
<td align="center"><code>sigmaTraitMu</code></td>
<td align="center"><code>mgibbs</code></td>
<td align="left"><span class="math inline">\(U_{m}U_{m'}\)</span> <span class="math inline">\(^7\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(^1\)</span> simulated object from <code>gjamSimData</code>.</p>
<p><span class="math inline">\(^2\)</span> fitted object from <code>gjamGibbs</code>.</p>
<p><span class="math inline">\(^3\)</span> sensitivities based on <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\Sigma\)</span>.</p>
<p><span class="math inline">\(^4\)</span> Only when <code>ydata</code> includes ordinal types.</p>
<p><span class="math inline">\(^5\)</span> Only with dimension reduction, <code>reductList</code> is included in <code>modelList</code> (Dimension reduction vignette).</p>
<p><span class="math inline">\(^6\)</span> Only for trait analysis, <code>traitList</code> is included in <code>modelList</code> (Trait vignette).</p>
<p><span class="math inline">\(^7\)</span> <span class="math inline">\(U\)</span> is the response data in the trait vignette.</p>
<p><br></p>
<p>The matrix <span class="math inline">\(\mathbf{F}\)</span> contains the covariance between predictors in <span class="math inline">\(\mathbf{X}\)</span> in terms of the responses <span class="math inline">\(\mathbf{Y}\)</span>. The diagonal <span class="math inline">\(\mathbf{f} = diag( \mathbf{F} )\)</span> is the sensitivity of the entire response matrix to each predictor in <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>The matrix <span class="math inline">\(\mathbf{E}\)</span> is the correlation among species in terms of their responses to <span class="math inline">\(\mathbf{X}\)</span>. Relationships to outputs are discussed in the <strong>Reference notes</strong>.</p>
</div>
<div id="examples-from-simulation" class="section level3">
<h3>Examples from simulation</h3>
<p>Simulated data are typical of real data in that there is a large fraction of zeros,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">bty =</span> <span class="st">'n'</span>, <span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">family=</span><span class="st">''</span>)
h &lt;-<span class="st"> </span><span class="kw">hist</span>(<span class="kw">c</span>(-<span class="dv">1</span>,f$y),<span class="dt">nclass =</span> <span class="dv">50</span>,<span class="dt">plot =</span> F)
<span class="kw">plot</span>(h$counts,h$mids,<span class="dt">type =</span> <span class="st">'s'</span>)
<span class="kw">plot</span>(f$w,f$y,<span class="dt">cex =</span> .<span class="dv">2</span>)</code></pre></div>
<p>Here is a short Gibbs sampler to estimate parameters and fit the data. The function <code>gjamGibbs</code> needs the <code>formula</code> for the model, the <code>data.frame xdata</code>, which includes the predictors, the response matrix <code>ydata</code>, and a <code>modelList</code> specifying number of Gibbs steps (<code>ng</code>), the <code>burnin</code>, and <code>typeNames</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># a few iterations</span>
ml  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">1000</span>, <span class="dt">burnin =</span> <span class="dv">100</span>, <span class="dt">typeNames =</span> f$typeNames)
out &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
<span class="kw">summary</span>(out)</code></pre></div>
<p>Among the objects to consider initially are the design matrix <code>out$x</code>, response matrix <code>out$y</code>, and the MCMC <code>out$chains</code> with these names and sizes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(out$chains)</code></pre></div>
<p><code>$chains</code> is a list of matrices, each with <code>ng</code> rows and as many columns as needed to hold parameter estimates. For example each row of <code>$chains$bgibbs</code> is a length-<span class="math inline">\(QS\)</span> vector of values for the <span class="math inline">\(Q \times S\)</span> matrix <span class="math inline">\(\mathbf{B}\)</span>. A row of <code>$chains$sgibbs</code> holds either the <span class="math inline">\(S(S + 1)/2\)</span> unique values of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> or the <span class="math inline">\(N \times r\)</span> unique values of the dimension reduced covariance matrix (see Dimension reduction <code>vignette</code>). A summary of the <code>chains</code> is given in Table 2.</p>
<p>Additional summaries are available in the list <code>modelSummary</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(out$modelSummary)</code></pre></div>
<p>The matrix <code>classBySpec</code> shows the number of observations in each interval. For this example of continuous data censored at zero, the two bins are <span class="math inline">\(k = 0, 1\)</span> corresponding to the intervals <span class="math inline">\((p_{s,0}, p_{s,1}] = (-\infty,0]\)</span> and <span class="math inline">\((p_{s,1}, p_{s,2}) = (0, \infty)\)</span>. The length-<span class="math inline">\((K + 1)\)</span> partition vector is the same for all species, <span class="math inline">\(\mathbf{p} = (-\infty, 0, \infty)\)</span>. Here is <code>classBySpec</code> for this example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out$modelSummary$classBySpec</code></pre></div>
<p>The first interval is censored (all values of <span class="math inline">\(y_{is}\)</span> = 0). The second interval is not censored (<span class="math inline">\(y_{is} = w_{is}\)</span>).</p>
<p>The fitted coefficients in <code>$parameterTables</code>, as summarized in Table 2. For example, here is posterior mean estimate of <span class="math inline">\(\mathbf{B}\)</span>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out$parameterTables$betaMu</code></pre></div>
<p>Here are the standard errors,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out$parameterTables$betaSe</code></pre></div>
<p>Again, check Table 2 for names of all fitted coefficients.</p>
<p>The data are also predicted in <code>gjamGibbs</code>, summarized by predictive means and standard errors. These are contained in <span class="math inline">\(n \times Q\)</span> matrices <code>$modelSummary$xpredMu</code> and <code>$modelSummary$xpredSd</code> and <span class="math inline">\(n \times S\)</span> matrices <code>$modelSummary$ypredMu</code> and <code>$modelSummary$ypredSd</code>. The estimates for latent states are included in <code>$modelSummary$wMu</code> and <code>$modelSummary$wSd</code>.</p>
<p>The output can be viewed with the function <code>gjamPlot</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f   &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">n =</span> <span class="dv">500</span>, <span class="dt">S =</span> <span class="dv">10</span>, <span class="dt">typeNames =</span> <span class="st">'CA'</span>)
ml  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">1000</span>, <span class="dt">burnin =</span> <span class="dv">200</span>, <span class="dt">typeNames =</span> f$typeNames)
out &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
pl  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues, <span class="dt">GRIDPLOTS =</span> T, <span class="dt">SMALLPLOTS =</span> F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
<p><code>gjamPlot</code> creates a number of plots comparing true and estimated parameters (for simulated data). Here are some simple biplots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">bty =</span> <span class="st">'n'</span>, <span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">family=</span><span class="st">''</span>)
<span class="kw">plot</span>(f$trueValues$beta, out$parameterTables$betaMu, <span class="dt">cex =</span> .<span class="dv">2</span>)
<span class="kw">plot</span>(f$trueValues$corSpec, out$parameterTables$corMu, <span class="dt">cex =</span> .<span class="dv">2</span>)
<span class="kw">plot</span>(f$y,out$modelSummary$ypredMu, <span class="dt">cex =</span> .<span class="dv">2</span>)</code></pre></div>
<p>To process the output beyond what is provided in <code>gjamPlot</code> I can work directly with the <code>chains</code>.</p>
</div>
<div id="my-data" class="section level3">
<h3>My data</h3>
<p><code>gjam</code> uses the standard <code>R</code> syntax in the <code>formula</code> that I would use with functions like <code>lm</code> and <code>glm</code>. Because <code>gjam</code> uses inverse prediction to summarize large multivariate output, it is important to abide by this syntax. For example, to analyze a model with quadratic and interaction terms, I might simply construct my own design matrix with these columns included, i.e., side-stepping the standard syntax for these effects that can be specified in <code>formula</code>. This would be fine for model fitting. However, without specifying this in the <code>formula</code> there is no way for <code>gjam</code> to know that these columns are in fact non-linear transformations of other columns. Without this knowledge there is no way to properly predict them. The prediction that <code>gjam</code> would return would include silly variable combinations.</p>
<p>To illustrate proper model specification I use a few lines from the <code>data.frame</code> of predictors in the <code>forestTraits</code> data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(repmis)
d &lt;-<span class="st"> &quot;https://github.com/jimclarkatduke/gjam/blob/master/forestTraits.RData?raw=True&quot;</span>
<span class="kw">source_data</span>(d)
xdata &lt;-<span class="st"> </span>forestTraits$xdata[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">8</span>)]</code></pre></div>
<p>Here are a few rows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xdata[<span class="dv">1</span>:<span class="dv">5</span>,]</code></pre></div>
<p>Here is a simple model specification with <code>as.formula()</code> that includes only main effects:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">formula &lt;-<span class="st"> </span><span class="kw">as.formula</span>( ~<span class="st"> </span>temp +<span class="st"> </span>deficit +<span class="st"> </span>soil )</code></pre></div>
<p>The design matrix <code>x</code> that is generated in <code>gjam</code> has an intercept, two covariates, and four columns for the multilevel factor <code>soil</code>:</p>
<pre><code>##   (Intercept)  temp deficit soilSpodHist soilEntVert soilMol soilUltKan
## 1           1  1.22    0.04            1           0       0          0
## 2           1  0.18    0.21            1           0       0          0
## 3           1 -0.94    0.20            0           0       0          0
## 4           1  0.64    0.82            1           0       0          0
## 5           1  0.82   -0.18            1           0       0          0</code></pre>
<p>To include interactions between temp and soil I use the symbol â€˜<code>*</code>â€™:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">formula &lt;-<span class="st"> </span><span class="kw">as.formula</span>( ~<span class="st"> </span>temp*soil )</code></pre></div>
<p>Here is the design matrix that results from this <code>formula</code> with interaction terms indicated by the symbol <code>':'</code></p>
<pre><code>##   (Intercept)  temp soilSpodHist soilEntVert soilMol soilUltKan
## 1           1  1.22            1           0       0          0
## 2           1  0.18            1           0       0          0
## 3           1 -0.94            0           0       0          0
## 4           1  0.64            1           0       0          0
## 5           1  0.82            1           0       0          0
##   temp:soilSpodHist temp:soilEntVert temp:soilMol temp:soilUltKan
## 1              1.22                0            0               0
## 2              0.18                0            0               0
## 3              0.00                0            0               0
## 4              0.64                0            0               0
## 5              0.82                0            0               0</code></pre>
<p>For a quadratic term I use the <code>R</code> function <code>I()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">formula &lt;-<span class="st"> </span><span class="kw">as.formula</span>( ~<span class="st"> </span>temp +<span class="st"> </span><span class="kw">I</span>(temp^<span class="dv">2</span>) +<span class="st"> </span>deficit )</code></pre></div>
<p>Here is the design matrix with linear and quadratic terms:</p>
<pre><code>##   (Intercept)  temp I(temp^2) deficit
## 1           1  1.22    1.4884    0.04
## 2           1  0.18    0.0324    0.21
## 3           1 -0.94    0.8836    0.20
## 4           1  0.64    0.4096    0.82
## 5           1  0.82    0.6724   -0.18</code></pre>
<p>Here is a quadratic response surface for <code>temp</code> and <code>deficit</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">formula &lt;-<span class="st"> </span><span class="kw">as.formula</span>( ~<span class="st"> </span>temp*deficit +<span class="st"> </span><span class="kw">I</span>(temp^<span class="dv">2</span>) +<span class="st"> </span><span class="kw">I</span>(deficit^<span class="dv">2</span>) )</code></pre></div>
<p>Here is the design matrix with all combinations:</p>
<pre><code>##   (Intercept)  temp deficit I(temp^2) I(deficit^2) temp:deficit
## 1           1  1.22    0.04    1.4884       0.0016       0.0488
## 2           1  0.18    0.21    0.0324       0.0441       0.0378
## 3           1 -0.94    0.20    0.8836       0.0400      -0.1880
## 4           1  0.64    0.82    0.4096       0.6724       0.5248
## 5           1  0.82   -0.18    0.6724       0.0324      -0.1476</code></pre>
<p>These are examples of the <code>formula</code> options available in <code>gjam</code>. Using them will allow for proper inverse prediction of <code>x</code>. To optimize MCMC, gjam does not predict <code>x</code> for higher order polynomialsâ€“they are rarely used, being both hard to interpret and generate unstable predictions. For such models set <code>predictX = F</code> in the <code>modelList</code>.</p>
<p>I can use this model to analyze a tree data set. For my data set I use the tree data contained in <code>forestTraits</code>. It is stored in de-zeroed format, so I extract it with the function <code>gjamReZero</code>. Here are dimensions and the upper left corner of the response matrix <span class="math inline">\(\mathbf{Y}\)</span>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gjam)
ydata  &lt;-<span class="st"> </span><span class="kw">gjamReZero</span>(forestTraits$treesDeZero)  <span class="co"># extract y</span>
<span class="kw">dim</span>(ydata)
ydata[<span class="dv">1</span>:<span class="dv">5</span>,<span class="dv">1</span>:<span class="dv">6</span>]</code></pre></div>
<p>In code that follows I treat them as discrete counts, <code>typeNames = 'DA'</code>. Because of the large number of columns (98) I speed things up calling for dimension reduction, passed as <span class="math inline">\(N \times r = 20 \times 8\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rl   &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">r =</span> <span class="dv">8</span>, <span class="dt">N =</span> <span class="dv">20</span>)
ml   &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">1000</span>, <span class="dt">burnin =</span> <span class="dv">100</span>, <span class="dt">typeNames =</span> <span class="st">'DA'</span>, <span class="dt">reductList =</span> rl)
form &lt;-<span class="st"> </span><span class="kw">as.formula</span>( ~<span class="st"> </span>temp*deficit +<span class="st"> </span><span class="kw">I</span>(temp^<span class="dv">2</span>) +<span class="st"> </span><span class="kw">I</span>(deficit^<span class="dv">2</span>) )
out  &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(form, <span class="dt">xdata =</span> xdata, <span class="dt">ydata =</span> ydata, <span class="dt">modelList =</span> ml)
pl   &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">SMALLPLOTS =</span> F, <span class="dt">GRIDPLOTS=</span>T, <span class="dt">corLines=</span>F, <span class="dt">specLabs =</span> F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
<p>Additional information on variable types and their treatment in <code>gjam</code> is included later in this document and in the other <code>gjam vignettes</code>.</p>
</div>
</div>
<div id="plotting-output" class="section level2">
<h2>Plotting output</h2>
<p>In the foregoing example arguments passed to <code>gjamPlot</code> in the <code>list plotPars</code> included <code>SMALLPLOTS = F</code> (do not compress margins and axes), <code>GRIDPLOTS = T</code> (draw grid diagrams as heat maps for parameter values and predictions), <code>corLines = F</code> (do not separate parameter values with lines on gridplots), and <code>specLabs = F</code> (do not put species labels on plots, because there are too many see clearly). In this section I summarize plots generated by <code>gjamPlot</code>.</p>
<p>By default, plots are rendered to the screen. I enter â€˜returnâ€™ to render the next plot. Faster execution obtains if I write plots directly to pdf files, with <code>SAVEPLOTS = T</code>. I can specify a folder this way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plotPars &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">SMALLPLOTS =</span> F, <span class="dt">GRIDPLOTS=</span>T, <span class="dt">SAVEPLOTS =</span> T, <span class="dt">outfolder =</span> <span class="st">'plots'</span>)</code></pre></div>
<p>In all plots, posterior distributions and predictions are shown as <span class="math inline">\(68\%\)</span> (boxes) and <span class="math inline">\(95\%\)</span> (whiskers) intervals, respectively. Here are the plots in alphabetical order by file name:</p>
<table style="width:112%;">
<colgroup>
<col width="8%"></col>
<col width="104%"></col>
</colgroup>
<thead>
<tr class="header">
<th align="right">Name</th>
<th align="left">Comments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><code>betaAll</code></td>
<td align="left">Posterior <span class="math inline">\(\mathbf{B}\)</span>; if <code>sigOnly</code> shows only 95% posteriors that exclude zero</td>
</tr>
<tr class="even">
<td align="right"><code>beta_(variable)</code></td>
<td align="left">Posterior distributions, one file per variable</td>
</tr>
<tr class="odd">
<td align="right"><code>betaChains</code></td>
<td align="left">Example MCMC chains for <span class="math inline">\(\mathbf{B}\)</span> (has it converged?)</td>
</tr>
<tr class="even">
<td align="right"><code>clusterDataE</code></td>
<td align="left">Cluster analysis of raw data and <span class="math inline">\(\textbf{E}\)</span> matrix</td>
</tr>
<tr class="odd">
<td align="right"><code>clusterGridB</code></td>
<td align="left">Cluster and grid plot of <span class="math inline">\(\mathbf{E}\)</span> and <span class="math inline">\(\mathbf{B}\)</span></td>
</tr>
<tr class="even">
<td align="right"><code>clusterGridE</code></td>
<td align="left">Cluster and grid plot of <span class="math inline">\(\mathbf{E}\)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>clusterGridR</code></td>
<td align="left">Cluster and grid plot of <span class="math inline">\(\mathbf{R}\)</span></td>
</tr>
<tr class="even">
<td align="right"><code>corChains</code></td>
<td align="left">Example MCMC chains for <span class="math inline">\(\textbf{R}\)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>dimRed</code></td>
<td align="left">Dimension reduction (see <code>vignette</code>) for <span class="math inline">\(\Sigma\)</span> matrix</td>
</tr>
<tr class="even">
<td align="right"><code>gridF_B</code></td>
<td align="left">Grid plot of sensitivity <span class="math inline">\(\mathbf{F}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>, ordered by clustering <span class="math inline">\(\mathbf{F}\)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>gridR_E</code></td>
<td align="left">Grid plot of <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\(\mathbf{E}\)</span> ordered by clustering <span class="math inline">\(\mathbf{R}\)</span></td>
</tr>
<tr class="even">
<td align="right"><code>gridR</code></td>
<td align="left">Grid plot of <span class="math inline">\(\mathbf{R}\)</span>, ordered by cluster analysis.</td>
</tr>
<tr class="odd">
<td align="right"><code>gridY_E</code></td>
<td align="left">Grid plot of correlation for data <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{E}\)</span>, ordered by clustering cor(<span class="math inline">\(\mathbf{Y}\)</span>)</td>
</tr>
<tr class="even">
<td align="right"><code>gridTraitB</code></td>
<td align="left">If traits are predicted, see <code>gjam vignette</code> on traits.</td>
</tr>
<tr class="odd">
<td align="right"><code>ordination</code></td>
<td align="left">PCA of <span class="math inline">\(\mathbf{E}\)</span> matrix, including eigenvalues (cumulative)</td>
</tr>
<tr class="even">
<td align="right"><code>partition</code></td>
<td align="left">If ordinal responses, posterior distribution of <span class="math inline">\(\mathcal{P}\)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>richness</code></td>
<td align="left">Predictive distribution with distribution of data (histogram)</td>
</tr>
<tr class="even">
<td align="right"><code>sensitivity</code></td>
<td align="left">Overall sensitivity <span class="math inline">\(\textbf{f}\)</span> by predictor</td>
</tr>
<tr class="odd">
<td align="right"><code>traits</code></td>
<td align="left">If traits are predicted, see <code>gjam vignette</code> on traits.</td>
</tr>
<tr class="even">
<td align="right"><code>traitPred</code></td>
<td align="left">If traits are predicted, see <code>gjam vignette</code> on traits.</td>
</tr>
<tr class="odd">
<td align="right"><code>trueVsPars</code></td>
<td align="left">If simulated data and <code>trueValues</code> included in <code>plotPars</code></td>
</tr>
<tr class="even">
<td align="right"><code>xPred</code></td>
<td align="left">Inverse predictive distribution of of <span class="math inline">\(\mathbf{X}\)</span></td>
</tr>
<tr class="odd">
<td align="right"><code>xPredFactors</code></td>
<td align="left">Inverse predictive distribution of factor levels</td>
</tr>
<tr class="even">
<td align="right"><code>yPred</code></td>
<td align="left">Predicted <span class="math inline">\(\mathbf{Y}\)</span>, in-sample (blue bars), out-of-sample (dots), and distribution of data (histogram)</td>
</tr>
<tr class="odd">
<td align="right"><code>yPredAll</code></td>
<td align="left">If <code>predictAllY</code> predict up to 16 species</td>
</tr>
</tbody>
</table>
<p>If the <code>plotPars</code> list passed to gjamPlot specifies <code>GRIDPLOTS = T</code>, then grid and clustur plots are generated as gridded values for <span class="math inline">\(\mathbf{B}\)</span>, <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and <span class="math inline">\(\mathbf{R}\)</span>. Gridplots of matrix <span class="math inline">\(\mathbf{R}\)</span> show conditional and marginal dependence in white and grey. In plots of <span class="math inline">\(\mathbf{E}\)</span> marginal independence is shown in grey, but conditional independence is not shown, as the matrix does not have an inverse (Clark et al. 2016).</p>
<p>The sensitivity matrix <span class="math inline">\(\mathbf{F}\)</span> is shown together in a plot with individual species responses <span class="math inline">\(\mathbf{B}\)</span>.</p>
<p>The plot in which the model residual correlation <span class="math inline">\(\mathbf{R}\)</span> and the response correlation <span class="math inline">\(\mathbf{B}\)</span> are compared are ordered by their similiarity in the <span class="math inline">\(\mathbf{R}\)</span>. If the two contain similar structure, then it will be evident in this comparison. There is no reason to expect them to be similar.</p>
<p>For large <span class="math inline">\(S\)</span> the labels are not shown on the graphs, they would be too small. The order of species and the cluster groups to which they belong are returned in <code>fit$clusterOrder</code> and <code>fit$clusterIndex</code>.</p>
</div>
<div id="flexibility-in-gjam" class="section level2">
<h2>Flexibility in gjam</h2>
<div id="heterogeneous-sample-effort" class="section level3">
<h3>Heterogeneous sample effort</h3>
<p>Here is an example with discrete abundance data, now with heterogeneous sample effort. Heterogeneous effort applies wherever plot area or search time varies, such as vegetation plots of varying area, animal survey data with variable search time, or catch returns from fishing vessels with different gear and trawling times. Here I simulate a list containing the columns and the effort that applies to those columns, shown for 50 observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S   &lt;-<span class="st"> </span><span class="dv">5</span>                             
n   &lt;-<span class="st"> </span><span class="dv">50</span>
ef  &lt;-<span class="st"> </span><span class="kw">list</span>( <span class="dt">columns =</span> <span class="dv">1</span>:S, <span class="dt">values =</span> <span class="kw">round</span>(<span class="kw">runif</span>(n,.<span class="dv">5</span>,<span class="dv">5</span>),<span class="dv">1</span>) )
f   &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(n, S, <span class="dt">typeNames =</span> <span class="st">'DA'</span>, <span class="dt">effort =</span> ef)
ef</code></pre></div>
<p>If <code>ef$values</code> consists of a length-n <code>vector</code>, then gjam assumes each value applies to all species in the corresponding row and column specified in the <code>vector ef$columns</code>. This is the case shown above and would apply when effort is plot area, search time, sample volumn, and so forth. Alternatively, <code>values</code> can be supplied as a <code>matrix</code>, which could differ by observation and species. For example, camera trap data detect large animals at greater distances than small animals. For simulation purposes <code>gjamSimData</code> only accepts a <code>vector</code>. However, for fitting with <code>gjamGibbs</code> <code>effort$values</code> can be supplied as a <code>matrix</code> with as many columns as are listed in <code>effort$columns</code>.</p>
<p>Because observations are discrete the continuous latent variables <span class="math inline">\(w_{is}\)</span> are censored. Unlike the previous continuous example, observations <span class="math inline">\(y_{is}\)</span> now assume only discrete values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(f$w,f$y, <span class="dt">cex =</span> .<span class="dv">2</span>)</code></pre></div>
<p>The large scatter reflects the variable effort represented by each observation. Incorporating the effort scale gives this plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(f$w*ef$values, f$y, <span class="dt">cex =</span> .<span class="dv">2</span>)</code></pre></div>
<p>The heterogeneous effort affects the weight of each observation in model fitting. The <code>effort</code> is entered in <code>modelList</code>. Increase the number of iterations and look at plots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S   &lt;-<span class="st"> </span><span class="dv">10</span>                             
n   &lt;-<span class="st"> </span><span class="dv">1500</span>
ef  &lt;-<span class="st"> </span><span class="kw">list</span>( <span class="dt">columns =</span> <span class="dv">1</span>:S, <span class="dt">values =</span> <span class="kw">round</span>(<span class="kw">runif</span>(n,.<span class="dv">5</span>,<span class="dv">5</span>),<span class="dv">1</span>) )
f   &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(n, S, <span class="dt">typeNames =</span> <span class="st">'DA'</span>, <span class="dt">effort =</span> ef)
ml  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">1000</span>, <span class="dt">burnin =</span> <span class="dv">250</span>, <span class="dt">typeNames =</span> f$typeNames, <span class="dt">effort =</span> ef)
out &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
pl  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues,<span class="dt">SMALLPLOTS=</span>F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
</div>
<div id="prior-distribution-on-coefficients" class="section level3">
<h3>Prior distribution on coefficients</h3>
<p>Informative prior distributions in regression models are rare. It can be important to use prior information, especially in the multivariate setting, where covariances between species can result in estimates where the sign of a coefficient effect makes no sense. Widespread use of non-informative prior distributions probably reflects not lack of prior knowledge, but rather a practical means for assigning magnitude and weight of the prior effect in a regression. In many cases the sign of the effect is known, but the magnitude is not. Imposing an informative prior distribution would necessarily require substantial ad hoc experimentation, which, at best, could only result in â€˜mostâ€™ of the posterior distribution occupying the desired positive or negative values.</p>
<p>The knowledge of the â€˜directionâ€™ of the effect can be readily implmented with truncated priors, having the advantage that the posterior distribution has the same shape as the likelihood, but restricted to positive or negative values (Clark et al. 2013).</p>
<p>The prior distribution for <span class="math inline">\(\mathbf{B}\)</span> is either non-informative (if unspecified) or truncated by limits provided in the <code>list betaPrior</code>. The <code>betaPrior list</code> contains the two matrices <code>loBeta</code> and <code>hiBeta</code>. The rows of these matrices have <code>rownames</code> that match explanatory variables in the <code>formula</code> and <code>colnames</code> in <code>xdata</code>. In the example that follows I fit a model for FIA data to winter temperature <code>temp</code>, climatic <code>deficit</code>, and local site <code>moisture</code> status. For this example I demonstrate a prior distribution for positive effects of warm winters and negative effects of climate deficit:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source_data</span>(<span class="st">&quot;https://github.com/jimclarkatduke/gjam/blob/master/forestTraits.RData?raw=True&quot;</span>)

xdata &lt;-<span class="st"> </span>forestTraits$xdata                    
y     &lt;-<span class="st"> </span><span class="kw">gjamReZero</span>(forestTraits$treesDeZero)  
ydata &lt;-<span class="st"> </span><span class="kw">gjamTrimY</span>(y,<span class="dv">300</span>)$y        <span class="co"># a sample of species</span>
types &lt;-<span class="st"> 'DA'</span>

xnames &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'temp'</span>,<span class="st">'deficit'</span>)      <span class="co"># variables for truncated priors</span>
Q      &lt;-<span class="st"> </span><span class="kw">length</span>(xnames)
S      &lt;-<span class="st"> </span><span class="kw">ncol</span>(ydata)

loBeta &lt;-<span class="st"> </span><span class="kw">matrix</span>(-<span class="ot">Inf</span>,Q,S)         <span class="co"># initialize priors</span>
hiBeta &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">Inf</span>,Q,S)
<span class="kw">rownames</span>(loBeta) &lt;-<span class="st"> </span><span class="kw">rownames</span>(hiBeta) &lt;-<span class="st"> </span>xnames

loBeta[<span class="st">'temp'</span>,]    &lt;-<span class="st"> </span><span class="dv">0</span>            <span class="co"># minimum zero</span>
hiBeta[<span class="st">'deficit'</span>,] &lt;-<span class="st"> </span><span class="dv">0</span>            <span class="co"># maximum zero</span>

bp &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">lo =</span> loBeta, <span class="dt">hi =</span> hiBeta)
rl &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="dv">10</span>, <span class="dt">r =</span> <span class="dv">5</span>)          <span class="co"># dimension reduction</span>
modelList &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">5000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> types, 
                  <span class="dt">betaPrior =</span> bp, <span class="dt">reductList =</span> rl)</code></pre></div>
<p>The combination of <code>loBeta</code> and <code>hiBeta</code> set the limits for posterior draws from the truncated multivariate normal distribution.</p>
</div>
<div id="sample-effort-in-composition-data" class="section level3">
<h3>Sample effort in composition data</h3>
<p>Composition count (<code>'CC'</code>) data have heterogenous effort due to different numbers of counts for each sample. For example, in microbiome data, the number of reads per sample can range from <span class="math inline">\(10^{2}\)</span> to <span class="math inline">\(10^{6}\)</span>. The number of reads does not depend on total abundance. It is generally agreed that only relative differences are important. gjam knows that the effort in <code>CC</code> data is the total count for the sample, so <code>effort</code> does not need to be specified. Here is an example with simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f     &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">S =</span> <span class="dv">8</span>, <span class="dt">typeNames =</span> <span class="st">'CC'</span>)
ml    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">2000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames)
out   &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
pl    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues, <span class="dt">SMALLPLOTS =</span> F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
<p>For comparison, here is an example with fractional composition, where there is no effort:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f     &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">S =</span> <span class="dv">20</span>, <span class="dt">typeNames =</span> <span class="st">'FC'</span>)
ml    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">2000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames)
out   &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
pl    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues, <span class="dt">SMALLPLOTS =</span> F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
<p>The default censoring for different data types can be changed. A <code>gjam vignette</code> on trait modeling provides an example.</p>
</div>
<div id="the-partition-in-ordinal-data" class="section level3">
<h3>The partition in ordinal data</h3>
<p>Ordinal count (<code>'OC'</code>) data are collected where abundance must be evaluated rapidly or precise measurements are difficult. Because there is no absolute scale the partition must be inferred. Here is an example with 10 species:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f   &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">typeNames =</span> <span class="st">'OC'</span>) 
ml  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">2000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames)
out &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
<span class="kw">print</span>(out)</code></pre></div>
<p>A simple plot of the posterior mean values of <code>cutMu</code> shows the estimates with true values from simulation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">keep &lt;-<span class="st"> </span><span class="kw">strsplit</span>(<span class="kw">colnames</span>(out$parameterTables$cutMu),<span class="st">'C-'</span>) <span class="co">#only saved columns</span>
keep &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">as.numeric</span>(<span class="kw">unlist</span>(keep)), <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> T)[,<span class="dv">2</span>]
<span class="kw">plot</span>(f$trueValues$cuts[,keep],out$parameterTables$cutMu)</code></pre></div>
<p>Here are plots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pl  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues, <span class="dt">SMALLPLOTS =</span> F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
</div>
<div id="categorical-data" class="section level3">
<h3>Categorical data</h3>
<p>Categorical data have levels within groups. The levels are unordered. The columns in <code>ydata</code> that hold categorical responses are not declared using the R function <code>factor</code>, but rather by <code>typeNames = &quot;CAT&quot;</code>. In observation vector <span class="math inline">\(\mathbf{y}_{i}\)</span> there are elements for one less than the number of factor levels. Suppose that observations are obtained on attributes of individual plants, each plant being an observation. The group <code>leaf</code> type might have four levels broadleaf decidious <code>bd</code>, needleleaf decidious <code>nd</code>, broadleaf evergreen <code>be</code>, and needleaf evergreen <code>ne</code>. A second group <code>xylem</code> anatomy might have three levels diffuse porous <code>dp</code>, ring porous <code>rp</code>, and tracheid <code>tr</code>. In both cases I assign the last class to be a reference class, <code>other</code>. Ten rows of the response matrix data might look like this:</p>
<pre><code>##    leaf xylem
## 1    be    dp
## 2    bd other
## 3    be    rp
## 4 other    dp
## 5    bd    dp
## 6    bd    rp
## 7    bd other</code></pre>
<p>This <code>data.frame ydata</code> becomes this response matrix <code>y</code>:</p>
<pre><code>##      leaf_bd leaf_nd leaf_be leaf_other xylem_dp xylem_rp xylem_other
## [1,]       0       0       1          0        1        0           0
## [2,]       1       0       0          0        0        0           1
## [3,]       0       0       1          0        0        1           0
## [4,]       0       0       0          1        1        0           0
## [5,]       1       0       0          0        1        0           0
## [6,]       1       0       0          0        0        1           0
## [7,]       1       0       0          0        0        0           1</code></pre>
<p><code>gjam</code> expands the two groups into four and three columns in <code>y</code>, respectively. As for composition data there is one redundant column for each group. Here is an example with simulated data, having two categorical groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">types &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'CAT'</span>,<span class="st">'CAT'</span>)
f     &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">n=</span><span class="dv">2000</span>, <span class="dt">S =</span> <span class="kw">length</span>(types), <span class="dt">typeNames =</span> types)
ml    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">1500</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames, <span class="dt">PREDICTX =</span> F)
out   &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>( f$formula, <span class="dt">xdata =</span> f$xdata, <span class="dt">ydata =</span> f$ydata, <span class="dt">modelList =</span> ml )
pl    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues, <span class="dt">SMALLPLOTS=</span>F, <span class="dt">plotAllY =</span> T)
<span class="kw">gjamPlot</span>(out, <span class="dt">plotPars =</span> pl)</code></pre></div>
</div>
<div id="combinations-of-data-types" class="section level3">
<h3>Combinations of data types</h3>
<p>One of the advantages of gjam is that it combines data of many types. Here is an example showing joint analysis of 12 species represented by five data types, specified by column:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">types &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'OC'</span>,<span class="st">'OC'</span>,<span class="st">'OC'</span>,<span class="st">'OC'</span>,<span class="st">'CC'</span>,<span class="st">'CC'</span>,<span class="st">'CC'</span>,<span class="st">'CC'</span>,<span class="st">'CC'</span>,<span class="st">'CA'</span>,<span class="st">'CA'</span>,<span class="st">'PA'</span>,<span class="st">'PA'</span>) 
f     &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">S =</span> <span class="kw">length</span>(types), <span class="dt">Q =</span> <span class="dv">3</span>, <span class="dt">typeNames =</span> types)
ml    &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">2000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames)
out   &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)
tmp   &lt;-<span class="st"> </span><span class="kw">data.frame</span>(f$typeNames, out$modelSummary$classBySpec[,<span class="dv">1</span>:<span class="dv">10</span>])
<span class="kw">print</span>(tmp)</code></pre></div>
<p>I have displayed the first 10 columns of <code>classBySpec</code> from the <code>modelSummary</code> of <code>out</code>, with their <code>typeNames</code>. The ordinal count (<code>'OC'</code>) data occupy lower intervals. The width of each interval in <code>OC</code> data depends on the estimate of the partition in <code>cutMu</code>.</p>
<p>The composition count (<code>'CC'</code>) data occupy a broader range of intervals. Because <code>CC</code> data are only relative, there is information on only <span class="math inline">\(S - 1\)</span> species. One species is selected as <code>other</code>. The <code>other</code> class can be a collection of rare species (Clark et al. 2016).</p>
<p>Both continuous abundance (<code>'CA'</code>) and presence-absence (<code>'PA'</code>) data have two intervals. For CA data only the first interval is censored, the zeros (see above). For <code>PA</code> data both interval are censored; it is a multivariate probit.</p>
<p>Here are some plots for analysis of this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pl  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">trueValues =</span> f$trueValues, <span class="dt">SMALLPLOTS =</span> F)
<span class="kw">gjamPlot</span>(<span class="dt">output =</span> out, <span class="dt">plotPars =</span> pl)</code></pre></div>
</div>
<div id="missing-data-out-of-sample-prediction" class="section level3">
<h3>Missing data, out-of-sample prediction</h3>
<p>gjam identifies missing values in <code>xdata</code> and <code>y</code> and models them as part of the posterior distribution. These are identified by the vector <code>missingIndex</code> as part of the output from <code>gjamGibbs</code>. The estimates for missing <span class="math inline">\(\mathbf{X}\)</span> are <code>missingX</code> and <code>missingXSd</code>. The estimates for missing <span class="math inline">\(\mathbf{Y}\)</span> are <code>yMissMu</code> and <code>yMissSd</code>.</p>
<p>To simulate missing data use <code>nmiss</code> to indicate the number of missing values. The actual value will be less than <code>nmiss</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">typeNames =</span> <span class="st">'OC'</span>, <span class="dt">nmiss =</span> <span class="dv">20</span>)
<span class="kw">which</span>(<span class="kw">is.na</span>(f$xdata), <span class="dt">arr.ind =</span> T)</code></pre></div>
<p>Note that missing values are assumed to occur in random rows and columns, but not in column one, which is the intercept and known. No further action is needed for model fitting, as <code>gjamGibbs</code> knows to treat these as missing data.</p>
<p>Out-of-sample prediction of <span class="math inline">\(\mathbf{Y}\)</span> is not part of the posterior distribution. For model fitting, holdouts can be specified randomly in <code>modelList</code> with <code>holdoutN</code> (the number of plots to be held out at random) or with <code>holdoutIndex</code> (observation numbers, i.e., row numbers in <code>x</code> and <code>y</code>). The latter can be useful when a comparison of predictions is desired for different models using the same plots as holdouts.</p>
<p>When observations are held out, <code>gjam</code> provides out-of-sample prediction for both <code>x[holdoutIndex,]</code> and <code>y[holdoutIndex,]</code>. The holdouts are not included in the fitting of <span class="math inline">\(\boldsymbol{B}\)</span>,<span class="math inline">\(\boldsymbol{\Sigma}\)</span>, or <span class="math inline">\(\mathcal{P}\)</span>. For prediction of <code>y[holdoutIndex,]</code>, the values of <code>x[holdoutIndex,]</code> are known, and sampling for <code>w[holdoutIndex,]</code> is done with multivariate normal distribution, without censoring. This is done because the censoring depends on <code>y[holdoutIndex,]</code>, which taken to be unknown. This sample of <code>w[holdoutIndex,]</code> becomes a prediction for <code>y[holdoutIndex,]</code> using the partition (Figure 1a).</p>
<p>For inverse prediction of <code>x[holdoutIndex,]</code> the values of <code>y[holdoutIndex,]</code> are known. This represents the situation where a sample of the community is available, and the investigator would like to predict the environment of origin.</p>
<p>Here is an example with simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f   &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">typeNames =</span> <span class="st">'CA'</span>, <span class="dt">nmiss =</span> <span class="dv">20</span>)
ml  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">2000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames, <span class="dt">holdoutN =</span> <span class="dv">50</span>)
out &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
xMu  &lt;-<span class="st"> </span>out$modelSummary$xpredMu
xSd  &lt;-<span class="st"> </span>out$modelSummary$xpredSd
yMu  &lt;-<span class="st"> </span>out$modelSummary$ypredMu
hold &lt;-<span class="st"> </span>out$holdoutIndex

<span class="kw">plot</span>(out$x[hold,-<span class="dv">1</span>],xMu[hold,-<span class="dv">1</span>], <span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="kw">title</span>(<span class="st">'holdouts in x'</span>); <span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(out$y[hold,], yMu[hold,], <span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="kw">title</span>(<span class="st">'holdouts in y'</span>); <span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre></div>
</div>
<div id="prediction-with-heterogenous-effort" class="section level3">
<h3>Prediction with heterogenous effort</h3>
<p>Out-of-sample prediction can not only be done by holding out samples in <code>gjamGibbs</code>. It can also be done post-fitting, with the function <code>gjamPredict</code>. In this second case, a prediction grid is passed together with the fitted object generated by <code>gjamGibbs</code>. Following an example using NEON data for counts of ground beetles and small mammals combined with continuous cover abundance of plants, I simulate, fit, and predict these data with heterogeneous sample effort:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sc  &lt;-<span class="st"> </span><span class="dv">3</span>                               <span class="co">#no. CA responses</span>
sd  &lt;-<span class="st"> </span><span class="dv">10</span>                              <span class="co">#no. DA responses</span>
tn  &lt;-<span class="st"> </span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">'CA'</span>,sc),<span class="kw">rep</span>(<span class="st">'DA'</span>,sd) )  <span class="co">#combine CA and DA obs</span>
S   &lt;-<span class="st"> </span><span class="kw">length</span>(tn)
n   &lt;-<span class="st"> </span><span class="dv">500</span>
emat   &lt;-<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">runif</span>(n,.<span class="dv">5</span>,<span class="dv">5</span>), n, sd)              <span class="co">#simulated DA effort</span>
effort &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">columns =</span> <span class="kw">c</span>((sc<span class="dv">+1</span>):S), <span class="dt">values =</span> emat )
f      &lt;-<span class="st"> </span><span class="kw">gjamSimData</span>(<span class="dt">n =</span> n, <span class="dt">typeNames =</span> tn, <span class="dt">effort =</span> effort)
ml     &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ng =</span> <span class="dv">2000</span>, <span class="dt">burnin =</span> <span class="dv">500</span>, <span class="dt">typeNames =</span> f$typeNames, <span class="dt">effort =</span> f$effort)
out    &lt;-<span class="st"> </span><span class="kw">gjamGibbs</span>(f$formula, f$xdata, f$ydata, <span class="dt">modelList =</span> ml)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dt">bty=</span><span class="st">'n'</span>)             
<span class="kw">gjamPredict</span>(out, <span class="dt">y2plot =</span> <span class="kw">colnames</span>(f$ydata)[tn ==<span class="st"> 'DA'</span>]) <span class="co">#predict DA data</span></code></pre></div>
<p>The prediction plot fits the data well, because it assumes the same effort. However, I might wish to predict data with a standard level of effort, say â€˜1â€™. This new effort is taken in the same units as was used to fit the data, e.g., plot area, time observed, and so on. I use the same design matrix, but specify this new effort. Here I first predict the data with the actual effort, followed by the new effort of 1</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdata   &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">xdata =</span> f$xdata, <span class="dt">effort=</span>effort, <span class="dt">nsim =</span> <span class="dv">50</span> )      <span class="co"># effort unchanged </span>
p1 &lt;-<span class="st"> </span><span class="kw">gjamPredict</span>(<span class="dt">output =</span> out, <span class="dt">newdata =</span> newdata)

<span class="kw">plot</span>(f$y[,tn ==<span class="st"> 'DA'</span>], p1$sdList$yMu[,tn ==<span class="st"> 'DA'</span>],<span class="dt">ylab =</span> <span class="st">'Predicted'</span>,<span class="dt">cex=</span>.<span class="dv">1</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)

newdata$effort$values &lt;-<span class="st"> </span>effort$values*<span class="dv">0</span> +<span class="st"> </span><span class="dv">1</span>       <span class="co"># predict for effort = 1</span>
p2 &lt;-<span class="st"> </span><span class="kw">gjamPredict</span>(<span class="dt">output =</span> out, <span class="dt">newdata =</span> newdata)

<span class="kw">points</span>(f$y[,tn ==<span class="st"> 'DA'</span>], p2$sdList$yMu[,tn ==<span class="st"> 'DA'</span>],<span class="dt">col=</span><span class="st">'orange'</span>,<span class="dt">cex=</span>.<span class="dv">1</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre></div>
<p>The orange dots show what the model would predict had effort on all observations been equal to 1.</p>
</div>
<div id="conditional-prediction" class="section level3">
<h3>Conditional prediction</h3>
<p><code>gjam</code> can predict a subset of columns in <code>y</code> conditional on other columns using the function <code>gjamPredict</code>. An example is provided in the <code>gjam vignette</code> on dimension reduction. Here is a second example using the model fitted in the previous section. Consider model prediction in the case where the second plant species is absent, and the first species is at its mean value. In other words, for these values for plant species, what is the effect of model predictors? I compare it to predictions where I first condition on the observed values for the first two plant species. I do not specify a new version of <code>xdata</code>, but rather include the columns of <code>y</code> to condition on:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdata &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ydataCond =</span> f$y[,<span class="dv">1</span>:<span class="dv">2</span>], <span class="dt">nsim=</span><span class="dv">200</span>)   <span class="co"># cond on obs CA data</span>
p1      &lt;-<span class="st"> </span><span class="kw">gjamPredict</span>(<span class="dt">output =</span> out, <span class="dt">newdata =</span> newdata)$sdList$yMu[,tn ==<span class="st"> 'DA'</span>]

yc     &lt;-<span class="st"> </span>f$y[,<span class="dv">1</span>:<span class="dv">2</span>]                                  <span class="co"># cond on new CA values</span>
yc[,<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">mean</span>(yc[,<span class="dv">1</span>])
yc[,<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
newdata   &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ydataCond =</span> yc, <span class="dt">nsim=</span><span class="dv">200</span>)
p2 &lt;-<span class="st"> </span><span class="kw">gjamPredict</span>(<span class="dt">output =</span> out, <span class="dt">newdata =</span> newdata)$sdList$yMu[,tn ==<span class="st"> 'DA'</span>]
<span class="kw">plot</span>(f$y[,tn ==<span class="st"> 'DA'</span>], p1, <span class="dt">xlab=</span><span class="st">'Obs'</span>, <span class="dt">ylab =</span> <span class="st">'Pred'</span>, <span class="dt">cex=</span>.<span class="dv">1</span>, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="kw">c</span>(p1,p2)))
<span class="kw">points</span>(f$y[,tn ==<span class="st"> 'DA'</span>], p2,<span class="dt">col=</span><span class="st">'orange'</span>,<span class="dt">cex=</span>.<span class="dv">1</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre></div>
<p>In the first case, I held the values for columns 1 and 2 at the values observed. In the second case, I conditioned on the specific values of <code>y</code> mentioned above. Both differ from the unconditional prediction.</p>
<p>When there are large covariances in the estimates of <span class="math inline">\(\Sigma\)</span> the conditional predictions can differ dramatically from anything observed. In fact, if I condition on values of y that are well outside the data predictions will make no sense at all. Conversely, if covariances in <span class="math inline">\(\Sigma\)</span> are near zero conditional distributions will not look much different from unconditional predictions. With dimension reduction we have only a crude estimate of <span class="math inline">\(\Sigma\)</span>, so conditional prediction was be judged accordingly.</p>
</div>
<div id="presence-only-data-with-effort" class="section level3">
<h3>Presence-only data with effort</h3>
<p>If I have a model for effort, then incidence data can have a likelihood, i.e., a probability assigned to observations that are aggregated into groups of known effort. I cannot model absence for the aggregate data unless I know how much effort was devoted to searching for it. Effort is widely known to have large spatio-temporal and taxonomic bias.</p>
<p>If I know the effort for a region, even in terms of a model (e.g., distance from universities and museums, from rivers or trails, numbers of a species already in collections), I can treat aggregate data as counts. If I do not know effort, out of desperation I might use the total numbers of all species counted in a region as a crude index of effort. The <code>help</code> page for function <code>gjamPoints2Grid</code> provides examples to aggregate incidence data with (x, y) locations to a lattice.</p>
<p>If effort is known I can supply a prediction grid <code>predGrid</code> for the known effort map and aggregate incidence to that map. I can then model the data are <code>'DA'</code> data, specifying effort as in the example above.</p>
<p>If effort is unknown, I can model the data as composition data, <code>'CC'</code>. Again, this is a desperate measure, because there are many reasons why even the total for all species at a lattice point might not represent relative effort.</p>
</div>
</div>
<div id="when-a-model-wont-execute" class="section level2">
<h2>When a model wonâ€™t execute</h2>
<p>A joint model for data sets with many response variables can be unstable for several reasons. Because the model can accommodate large, multivariate data sets, there is temptation to throw everything in and see what happens. gjam is vulnerable due to the fact that columns in <code>ydata</code> have different scales and, thus, can range over orders of magnitude. Itâ€™s best to start small, gain a feel for the data and how it translates to estimates of many coefficients and covariances. More species and predictors can be added without changing the model. The opposite approach of throwing in everything is asking for trouble and is unlikely to generate insight.</p>
<p>If execution fails there are several options.</p>
<p>If you are simulating data, first try it again. The simulator aims to generate data that will actually work, more challenging than would be the case for a univariate simulation of a single data type. Simulated data are random, so try again.</p>
<p>If the fit is bad, it could be noisy data (there is no â€˜signalâ€™), but there are other things to check. Insure that all columns in <code>ydata</code> include at least some non-zero values. One would not expect a univariate model to fit a data set where <code>ydata</code> is all zeros. However, when there are many columns in <code>ydata</code>, the fact that some are never or rarely observed can be overlooked. The functions <code>hist</code>, <code>colSums</code>, and, for discrete data, <code>table</code>, can be used. The function <code>gjamTrimY(ydata, m)</code> can be used to limit <code>ydata</code> to only those columns with at least <code>m</code> non-zero observations.</p>
<p>Large differences in scale in <code>ydata</code> can contribute to instability. Unlike <code>xdata</code>, where the design matrix is standardized, <code>ydata</code> is not rescaled. It is used as-is, because the user may want effort on a specific scale. However, the algorithm is most stable when responses in <code>ydata</code> do not span widely different ranges. For continuous data (<code>&quot;CA&quot;</code>, <code>&quot;CON&quot;</code>), consider changing the units in <code>ydata</code> from, say, g to kg or from g ml<span class="math inline">\(^{-1}\)</span> to g l<span class="math inline">\(^{-1}\)</span>. For discrete counts (<code>&quot;DA&quot;</code>) consider changing the units for effort, e.g., m<span class="math inline">\(^{2}\)</span> to ha or hours to days. Rescaling is not relevant for <code>&quot;CC&quot;</code>, where modeling is done on the <span class="math inline">\([0, 1]\)</span> scale.</p>
<p>Unlike experiments, where attention is paid to balanced design, observational data often involve factors, for which only some species occur in all factor combinations. This inadequate distribution of data is compounded when those factors are included in interaction terms. Consider ways to eliminate factor levels/combinations that cannot be estimated from the data.</p>
<p>If a simulation fails due to a cholesky error (<span class="math inline">\(\boldsymbol{\Sigma}\)</span> is not positive definite), consider either reducing the number of columns in <code>ydata</code> or implementing dimension reduction (see the gjam vignette on this subject).</p>
</div>
<div id="reference-notes" class="section level1">
<h1>Reference notes</h1>
<div id="parameter-dimensions" class="section level3">
<h3>Parameter dimensions</h3>
<p>Unlike a univariate model that has one <span class="math inline">\(Y\)</span> per observation or multivariate models where all <span class="math inline">\(Y\)</span>â€™s have the same scale, gjam has <span class="math inline">\(Y\)</span>â€™s on multiple scales. So there are two sets of scales to consider, the scales for the <span class="math inline">\(X\)</span>â€™s and those for the <span class="math inline">\(Y\)</span>â€™s. To avoid more notation I just refer to the scale of a coefficient in <span class="math inline">\(\mathbf{B}_u\)</span> for a model having unstandardized variables as <span class="math inline">\(W/X\)</span> (Table 2). Except for responses that have no scale (presence-absence, ordinal) <span class="math inline">\(W\)</span> has the same dimension as <span class="math inline">\(Y\)</span>. For this reason, unstandardized coefficients are not prone to be misinterpreted by a user who has supplied unstandardized predictors in <code>xdata</code>. I refer to the corresponding unstandardized design matrix as <span class="math inline">\(\mathbf{X}_u\)</span>. Variables returned by <code>gjamGibbs</code>, including MCMC chains in <code>$chains$bgibbs</code> and posterior means and standard errors <code>$parameterTable$betaMu</code> and <code>$parameterTable$betaSd</code> are are on the scale provided by the user in <code>xdata</code>. Each row of <code>$chains$bgibbs</code> is one draw from the <span class="math inline">\(Q \times S\)</span> matrix <span class="math inline">\(\mathbf{B}_u\)</span>.</p>
<p>Models are commonly fitted to covariates <span class="math inline">\(X\)</span> that are â€˜standardizedâ€™ for mean and variance. Standardization can stabilize posterior simulation. It is desirable when coefficients are needed in standard deviations. Inside <code>gjamGibbs</code> design matrix <span class="math inline">\(\mathbf{X}\)</span>, and thus <span class="math inline">\(\mathbf{B}\)</span>, are standardized, thus having dimension <span class="math inline">\(W\)</span>, not <span class="math inline">\(W/X\)</span>. Of course, for variables in <code>xdata</code> supplied in standarized form, <span class="math inline">\(\mathbf{B} = \mathbf{B}_u\)</span>. See <strong>Algorithm summary</strong>.</p>
<p>The third set of scales comes for the correlation scale for the <span class="math inline">\(W\)</span>â€™s. The correlation scale can be useful when considering responses that have different scales. In addition to <span class="math inline">\(\mathbf{B}_u\)</span> we provide parameters on the correlation scale. This correlation scale is <span class="math inline">\(\mathbf{B}_r = \mathbf{B} \mathbf{D}^{-1/2}\)</span>, where <span class="math inline">\(\mathbf{D} = diag(\boldsymbol{\Sigma})\)</span>. If <span class="math inline">\(X\)</span> is standardized, <span class="math inline">\(\mathbf{B}_r\)</span> is dimensionless. The MCMC chains in <code>$chains$fbgibbs</code> and the estimates in <code>$parameterTable$fBetaMu</code> and <code>$parameterTable$fBetaSd</code> are standardized for <span class="math inline">\(X\)</span> (standard deviation scale) and for <span class="math inline">\(W\)</span> (correlation scale). They are dimensionless.</p>
<p>For sensitivity over all species and for comparisons between predictors we provide standardized sensitivity in a length-<span class="math inline">\(Q\)</span> vector <span class="math inline">\(\mathbf{f}\)</span>. The sensitivity matrix to <span class="math inline">\(X\)</span> across the full model is given by</p>
<p><span class="math display">\[\mathbf{F} = \mathbf{B}\boldsymbol{\Sigma}^{-1} \mathbf{B}'\]</span></p>
<p>Note that F takes <span class="math inline">\(\mathbf{B}\)</span>, not <span class="math inline">\(\mathbf{B}_r\)</span>, not <span class="math inline">\(\mathbf{B}_u\)</span>. This is <code>$parameterTables$fmatrix</code>. The sensitivity vector is <span class="math inline">\(\mathbf{f} = diag( \mathbf{F} )\)</span>. This is the <code>vector $parameterTables$fMu</code>. Details are given in Clark et al. (2016).</p>
</div>
<div id="when-there-are-factors-in-mathbfx" class="section level3">
<h3>When there are factors in <span class="math inline">\(\mathbf{X}\)</span></h3>
<p>The coefficient matrix <span class="math inline">\(\boldsymbol{\tilde{\mathbf{B}}}\)</span> is useful when there are <strong>factors</strong> in the model. Factor treatment in <code>gjam</code> follows the convention where a reference level is taken as the overall intercept, and remaining coefficients are added to the intercept. This approach makes sense in the ANOVA context, where an experiment has a control level to which other treatment levels are to be compared. A â€˜significantâ€™ level is different from the reference (e.g., control), but we are not told about its relationship to other levels. The coefficients in <code>$parameterTables$betaMu</code> and <code>$parameterTables$betaSd</code> are reported this way. Should it be needed the contrasts matrix for this design is returned as a list for all factors in the model as <code>$modelSummary$contrasts</code> and as a single contrasts matrix for the entire model as <code>$modelSummary$eCont</code>.</p>
<p>This standard structure is not the best way to compare factors in many ecological data sets, where a factor might represent soil type, cover type, fishing gear, biome, and so on. In all of these cases there is no â€˜controlâ€™ treatment. Here it is more useful to know how all levels relate to the mean taken across factor levels.</p>
<p>To provide more informative comparisons across factor levels and species we introduce a <span class="math inline">\(Q_1 \times S\)</span> recontrast matrix that translates <span class="math inline">\(\mathbf{B}\)</span>, with intercept, to all factor levels, without intercept.</p>
<p>Consider a three-level factor with levels <code>a, b, c</code>, the first being the reference class. There is a matrix <span class="math inline">\(\mathbf{G}\)</span></p>
<pre><code>##   intercept  b  c
## a         1 -1 -1
## b         1  1  0
## c         1  0  1</code></pre>
<p>and a matrix <span class="math inline">\(\mathbf{H}\)</span>,</p>
<pre><code>##            a b c
## intercept  1 0 0
## b         -1 1 0
## c         -1 0 1</code></pre>
<p>With <span class="math inline">\(\mathbf{L'} = \mathbf{G}^{-1}\)</span>, the recontrasted coefficients are</p>
<p><span class="math display">\[\mathbf{\tilde{B}} = \mathbf{L}\mathbf{B}\]</span></p>
<p>The rows of <span class="math inline">\(\mathbf{\tilde{B}}\)</span> correspond to all factor levels. The intercept does not appear, because it has been distributed across factor levels. The corresponding design matrix is</p>
<p><span class="math display">\[\mathbf{\tilde{X}} = \mathbf{X}\mathbf{H}\]</span></p>
<p>If there are multiple factors then <span class="math inline">\(Q_1 &gt; Q\)</span>, because the intercept expands to the reference classes for each factor. Should they be of interest the contrasts matrices for all factors are contained in the <code>list $modelSummary$contrasts</code>, and that for the full model <span class="math inline">\(\mathbf{C}\)</span> is <code>$modelSummary$eCont</code>. <span class="math inline">\(\mathbf{L}\)</span> is <code>$modelSummary$lCont</code>.</p>
<p>With factors, the sensitivity matrix reported in <code>$parameterTables$fmatrix</code>is</p>
<p><span class="math display">\[\mathbf{F} = \mathbf{\tilde{B}}\boldsymbol{\Sigma}^{-1} \mathbf{\tilde{B}'}\]</span></p>
<p>The response matrix in <code>$parameterTables$ematrix</code> is</p>
<p><span class="math display">\[\mathbf{E} = \mathbf{\tilde{B}} \mathbf{\tilde{V}} \mathbf{\tilde{B}'}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\tilde{V}} = cov \left(\mathbf{X} \mathbf{H} \right)\)</span>.</p>
</div>
<div id="algorithm-summary" class="section level2">
<h2>Algorithm summary</h2>
<p>Model fitting is done by Gibbs sampling. The design matrix <span class="math inline">\(\mathbf{X}\)</span> is centered and standardized. Parameters <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are sampled directly,</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\boldsymbol{\Sigma}|\mathbf{W}, \mathbf{B}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{B}| \boldsymbol{\Sigma}, \mathbf{W}\)</span></p></li>
<li><p>For unknown partition (ordinal variables) the partition is sampled, <span class="math inline">\(\mathcal{P}|\mathbf{Z}, \mathbf{W}\)</span></p></li>
<li><p>For ordinal, presence-absence, and categorical data, latent variables are drawn on the correlation scale, <span class="math inline">\(\mathbf{W}|\mathbf{R}, \boldsymbol{\alpha}, \mathbf{P}\)</span>, where <span class="math inline">\(\mathbf{R} = \mathbf{D}^{-1/2}\boldsymbol{\Sigma}\mathbf{D}^{-1/2}\)</span>, <span class="math inline">\(\boldsymbol{\alpha} = \mathbf{D}^{-1/2}\mathbf{B}\)</span>, <span class="math inline">\(\mathbf{P} = \mathbf{D}^{-1/2}\mathcal{P}\)</span>, and <span class="math inline">\(\mathbf{D} = diag(\boldsymbol{\Sigma})\)</span>.<br />
For other variables that are discrete or censored, latent variables are drawn on the covariance scale, <span class="math inline">\(\mathbf{W}| \boldsymbol{\Sigma}, \mathbf{B}, \mathcal{P}\)</span>.</p></li>
</ol>
<p>Parameters in <code>$chains$bgibbs</code> are returned on the original scale <span class="math inline">\(\mathbf{X}_u\)</span>. Let <span class="math inline">\(\mathbf{X}_u\)</span> be the uncentered/unstandardized version of <span class="math inline">\(\mathbf{X}\)</span>. Parameters are returned as <span class="math inline">\(\mathbf{B}_u = \left(\mathbf{X}'_u \mathbf{X}_u \right)^{-1}\mathbf{X}'_u \mathbf{X}\mathbf{B}\)</span>. Likewise, <code>$x</code> is returned on the original scale, i.e., it is <span class="math inline">\(\mathbf{X}_u\)</span>.</p>
<p>Inverse prediction of input variables provides sensitivity analysis (Clark et al. 2011, 2014). Columns in <span class="math inline">\(\mathbf{X}\)</span> that are linear (not involved in interactions, polynomial terms, or factors) are sampled directly from the inverted model. Others are sampled by Metropolis. Sampling is described in the Supplement file to Clark et al. (2016).</p>
<p>For additional information see this <a href="http://sites.nicholas.duke.edu/clarklab/code/">link</a></p>
<p>The model is described in Clark et al (2016).</p>
</div>
</div>
<div id="acknowledgements" class="section level1">
<h1>Acknowledgements</h1>
<p>For valuable feedback on the model and computation I thank Bene Bachelot, Alan Gelfand, Diana Nemergut, Erin Schliep, Bijan Seyednasrollah, Daniel Taylor-Rodriquez, Brad Tomasek, Phillip Turner, and Stacy Zhang. I thank the members of NSFâ€™s <em>SAMSI</em> program on <em>Ecological Statistics</em> and my class <em>Bayesian Analysis of Environmental Data</em> at Duke University.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Brynjarsdottir, J. and A.E. Gelfand. 2014. Collective sensitivity analysis for ecological regression models with multivariate response. <em>Journal of Biological, Environmental, and Agricultural Statistics</em>, 19, 481-502.</p>
<p>Chib, S. and E. Greenberg. 1998. Analysis of multivariate probit models. <em>Biometrika</em> 85, 347-361.</p>
<p>Clark, J.S., D.M. Bell, M.H. Hersh, and L. Nichols. 2011. Climate change vulnerability of forest biodiversity: climate and resource tracking of demographic rates. <em>Global Change Biology</em>, 17, 1834-1849.</p>
<p>Clark, J.S., D. M Bell, M. Kwit, A. Powell, And K. Zhu. 2013. Dynamic inverse prediction and sensitivity analysis with high-dimensional responses: application to climate-change vulnerability of biodiversity. <em>Journal of Biological, Environmental, and Agricultural Statistics</em>, 18, 376-404.</p>
<p>Clark, J.S., A.E. Gelfand, C.W. Woodall, and K. Zhu. 2014. More than the sum of the parts: forest climate response from joint species distribution models. <em>Ecological Applications</em> 24, 990-999</p>
<p>Clark, J.S., D. Nemergut, B. Seyednasrollah, P. Turner, and S. Zhang. 2016. Generalized joint attribute modeling for biodiversity analysis: Median-zero, multivariate, multifarious data, <em>Ecological Monographs</em>, in press.</p>
<p>Lawrence, E., D. Bingham, C. Liu and V. N. Nair (2008) Bayesian inference for multivariate ordinal data using parameter expansion. <em>Technometrics</em> 50, 182-191.</p>
<p>Zhang, X., W.J. Boscardin, and T.R. Belin. 2008. Bayesian analysis of multivariate nominal measures using multivariate multinomial probit models. <em>Computational Statistics and Data Analysis</em> 52, 3697-3708.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
